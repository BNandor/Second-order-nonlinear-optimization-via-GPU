{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMHH for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "sys.path.insert(0, '../..')\n",
    "sys.path.insert(0, '../../..')\n",
    "sys.path.insert(0, '../../../..')\n",
    "import runExperiment.hyperParameterTuning.pyNMHH.pyNMHH as pyNMHH\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor\n",
    "\n",
    "def initialClassificationBaseLevelConfig():\n",
    "    return {\n",
    "    \"OperatorsAndCategories\":{\n",
    "        \"categories\": ['perturbator', 'selector','refiner'],\n",
    "        \"operators\": {\n",
    "            'perturbator': ['DE', 'GA'],\n",
    "            'selector': ['best'],\n",
    "            'refiner': ['BayesGP']\n",
    "        }\n",
    "    },\n",
    "    \"Restrictions\":{\n",
    "        \"CategoryTransitionRestrictions\":[[0],[1,2],[0]]\n",
    "    },\n",
    "    \"CategoryTransitionMatrix\": {\n",
    "        \"InitialProbabilities\": [1, 0.0,0.0],\n",
    "        \"TransitionMatrix\": [\n",
    "            [0.0, 0.5,0.5],\n",
    "            [1.0, 0.0,0.0],\n",
    "            [0.0,1.0,0.0]\n",
    "        ]\n",
    "    },\n",
    "    \"OperatorTransitionMatrices\": {\n",
    "        \"perturbator\": {\n",
    "            \"InitialProbabilities\": [0.705285341737996, 0.29471465826200405],\n",
    "            \"TransitionMatrix\": [\n",
    "                [0.8062957200961741, 0.19370427990382585],\n",
    "                [1.0, 0.0]\n",
    "            ]\n",
    "        },\n",
    "        \"selector\": {\n",
    "            \"InitialProbabilities\": [1.0],\n",
    "            \"TransitionMatrix\": [[1.0]]\n",
    "        },\n",
    "        \"refiner\": {\n",
    "            \"InitialProbabilities\": [1.0],\n",
    "            \"TransitionMatrix\": [[1.0]]\n",
    "        }\n",
    "    },\n",
    "    \"OperatorParams\":{\n",
    "        \"PerturbatorDEOperatorParams\": {\n",
    "                        \"DE_CR\": {\n",
    "                            \"lowerBound\": 0.0,\n",
    "                            \"upperBound\": 1.0,\n",
    "                            \"value\": 1.0\n",
    "                        },\n",
    "                        \"DE_FORCE\": {\n",
    "                            \"lowerBound\": 0.0,\n",
    "                            \"upperBound\": 2.0,\n",
    "                            \"value\": 0.566329910553018\n",
    "                        }\n",
    "                    },\n",
    "        \"PerturbatorGAOperatorParams\": {\n",
    "                        \"GA_ALPHA\": {\n",
    "                            \"lowerBound\": 0.0,\n",
    "                            \"upperBound\": 1.0,\n",
    "                            \"value\": 0.0\n",
    "                        },\n",
    "                        \"GA_CR\": {\n",
    "                            \"lowerBound\": 0.0,\n",
    "                            \"upperBound\": 1.0,\n",
    "                            \"value\": 0.23283148368816325\n",
    "                        },\n",
    "                        \"GA_CR_POINT\": {\n",
    "                            \"lowerBound\": 0.0,\n",
    "                            \"upperBound\": 1.0,\n",
    "                            \"value\": 0.9979030839478561\n",
    "                        },\n",
    "                        \"GA_MUTATION_RATE\": {\n",
    "                            \"lowerBound\": 0.0,\n",
    "                            \"upperBound\": 0.5,\n",
    "                            \"value\": 0.0\n",
    "                        },\n",
    "                        \"GA_MUTATION_SIZE\": {\n",
    "                            \"lowerBound\": 0.0,\n",
    "                            \"upperBound\": 100.0,\n",
    "                            \"value\": 100.0\n",
    "                        },\n",
    "                        \"GA_PARENTPOOL_RATIO\": {\n",
    "                            \"lowerBound\": 0.2,\n",
    "                            \"upperBound\": 1.0,\n",
    "                            \"value\": 0.707368716697075\n",
    "                        }\n",
    "                    }\n",
    "    }\n",
    "}\n",
    "\n",
    "def k_fold_split(X, y, k, shuffle=True, random_state=None):\n",
    "    kf = KFold(n_splits=k, shuffle=shuffle, random_state=random_state)\n",
    "    \n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(X), 1):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        yield fold, X_train, y_train, X_test, y_test\n",
    "\n",
    "class FlatHyperParameters:\n",
    "    def __init__(self,params):\n",
    "        self.params=params\n",
    "        self.lowerBounds={}\n",
    "        self.upperBounds={}        \n",
    "        for key,value in params.items():\n",
    "            if isinstance(value[0], (int, float)):\n",
    "                self.lowerBounds[key]=float(value[0])\n",
    "                self.upperBounds[key]=float(value[1])\n",
    "            else:\n",
    "                self.lowerBounds[key]=0\n",
    "                self.upperBounds[key]=len(value)-1        \n",
    "    def unflatten(self,flatParameters):\n",
    "        original_representation = {}\n",
    "        for (key, param_value), value in zip(self.params.items(), flatParameters):\n",
    "            if value<self.lowerBounds[key]:\n",
    "                boundedValue = self.lowerBounds[key]\n",
    "            else:\n",
    "                if value>self.upperBounds[key]:\n",
    "                    boundedValue = self.upperBounds[key]\n",
    "                else:\n",
    "                    boundedValue = value \n",
    "            if isinstance(param_value[0], (int)):\n",
    "                original_representation[key] = int(boundedValue)\n",
    "            else:\n",
    "                if isinstance(param_value[0], (float)):\n",
    "                    original_representation[key] = boundedValue\n",
    "                else:\n",
    "                    original_representation[key] = param_value[int(boundedValue)]\n",
    "        return original_representation\n",
    "        \n",
    "    def flatten(self,params):\n",
    "        values = []\n",
    "        for (key,value) in params.items():\n",
    "            if isinstance(value[0], int) or isinstance(value[0], float)  :\n",
    "                values.append(value)\n",
    "            else:\n",
    "                values.append(params[key].index(value))\n",
    "        return values\n",
    "\n",
    "    def randomUniformSample(self):\n",
    "        sample = []\n",
    "        for (key,bounds) in self.params.items():\n",
    "            if isinstance(bounds[0], (int, float)):\n",
    "                sample.append(pyNMHH.random.uniform(self.lowerBounds[key], self.upperBounds[key]))\n",
    "            else:\n",
    "                sample.append(pyNMHH.random.randint(self.lowerBounds[key], self.upperBounds[key]))\n",
    "        return sample\n",
    "    \n",
    "    def flatBounds(self):\n",
    "        lower = []\n",
    "        upper = []\n",
    "        for (key,bounds) in self.params.items():\n",
    "            lower.append(self.lowerBounds[key])\n",
    "            upper.append(self.upperBounds[key])\n",
    "        return lower,upper\n",
    "    \n",
    "    def xtypes(self):\n",
    "        types = []\n",
    "        for (key,bounds) in self.params.items():\n",
    "            if isinstance(bounds[0], (float)):\n",
    "                types.append('continuous')\n",
    "            else:\n",
    "                types.append('discrete')\n",
    "        return types\n",
    "    \n",
    "class TrainableModelWrapper:\n",
    "    def __init__(self, X, y, fraction, classifier,predictor, classifier_name, dataset_name,hyperParamConfig,cv=3):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.fraction=fraction\n",
    "        self.classifier=classifier\n",
    "        self.predictor=predictor\n",
    "        self.classifier_name=classifier_name\n",
    "        self.dataset_name=dataset_name\n",
    "        self.hyperParamConfig=hyperParamConfig\n",
    "        self.cv=cv\n",
    "\n",
    "    def __call__(self, flatHyperParams):\n",
    "        return self.train_and_evaluate(self.X, self.y, self.fraction, self.classifier,self.predictor, self.hyperParamConfig.unflatten(flatHyperParams), self.classifier_name, self.dataset_name)\n",
    "    \n",
    "    def train_and_evaluate(self, X, y, fraction, classifier,predictor, params, classifier_name, dataset_name, num_iterations=1,foldLimit=10):\n",
    "        accuracies = []\n",
    "        crossvalcount=int(1/(fraction))\n",
    "        for i in  range(num_iterations):\n",
    "            if crossvalcount >=2:\n",
    "                for fold, X_train, y_train, X_test, y_test in k_fold_split(X, y, k=crossvalcount):\n",
    "                    if fold > foldLimit:\n",
    "                        continue\n",
    "                    print(f\"                    >>>>>training: {classifier_name} with {params} on {dataset_name} at training data size fraction of {fraction} at iteration: {i+1}/{num_iterations}\")\n",
    "                    clf = classifier(**params)\n",
    "                    clf.fit(X_test, y_test)\n",
    "                    y_pred = predictor(clf,X_train)\n",
    "                    accuracy = accuracy_score(y_train, y_pred)\n",
    "                    accuracies.append(accuracy)\n",
    "            else:\n",
    "                clf = classifier(**params)\n",
    "                scores = cross_val_score(clf, X, y,cv=self.cv,scoring='accuracy',n_jobs=-1)\n",
    "                accuracies.append(scores.mean())\n",
    "                print(f\"                    >>>>>trained: {classifier_name} with {params} on {dataset_name} crossvalidationCount: {self.cv} at iteration: {i+1}/{num_iterations}-> scores:{scores}/mean{scores.mean()}\")\n",
    "                # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1-fraction, random_state=None)\n",
    "                \n",
    "                # clf = classifier(**params)\n",
    "                # clf.fit(X_train, y_train)\n",
    "                \n",
    "                # y_pred = predictor(clf,X_test)\n",
    "                #     # y_pred = clf.predict(X_train)\n",
    "                # accuracy = accuracy_score(y_test, y_pred)\n",
    "                # accuracies.append(accuracy)\n",
    "       \n",
    "        return -(pyNMHH.np.array(accuracies).mean())\n",
    "    \n",
    "class BaseLevel():\n",
    "\n",
    "    def __init__(self,populationSize,max_evaluations, X, y, fraction, classifier,predictor, classifier_name, dataset_name,classifierConfig,cv=3):\n",
    "        self.trainableModel=TrainableModelWrapper(X, y, fraction, classifier,predictor, classifier_name, dataset_name,classifierConfig,cv)\n",
    "        self.classifierConfig=classifierConfig\n",
    "        self.population_size=populationSize\n",
    "        self.max_evaluations=max_evaluations\n",
    "\n",
    "    def __call__(self,baseLevelConfig, history):\n",
    "        lower,upper=self.classifierConfig.flatBounds()\n",
    "        types=self.classifierConfig.xtypes()\n",
    "        wrapped_objective = pyNMHH.FunctionWrapper( self.trainableModel, self.max_evaluations,1000000,lowerbounds=lower,upperbounds=upper,xtypes=types)\n",
    "        population = [pyNMHH.Individual(self.classifierConfig.randomUniformSample()) for _ in range(self.population_size)]\n",
    "        # prevsize=min(population_size,len(history.best_offspring))\n",
    "        # population = [Individual(self.classifierConfig.randomUniformSample()) for _ in range(population_size-prevsize)]\n",
    "        # if (len(history.best_offspring)>0):\n",
    "        #     [population.append(Individual(prev)) for prev in history.best_offspring[-prevsize:]]\n",
    "        # else:\n",
    "        #     population.append(Individual(self.classifierConfig.randomUniformSample()))\n",
    "        # try:\n",
    "        best_solution, best_fitness, total_evaluations = pyNMHH.NMHHBaseOpt(wrapped_objective, population, self.max_evaluations, baseLevelConfig, history)\n",
    "        return {\"fitness\":best_fitness,\"solution\":best_solution}\n",
    "        # except Exception as e:\n",
    "        #     print(f\"Error in optimization: {e}\")\n",
    "        #     return float('inf')  # Return a high value if there's an error\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run classification experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runClassificationTuning(config):\n",
    "    print(f\"            >>>pyNMHH optimization starting for {config['classifierName']}-{config['datasetName']}\")\n",
    "    best_params, best_fitness, history,bestSequence,best_solution = pyNMHH.NMHHHyperOpt(config['baseLevelConfig'], \n",
    "                                                    BaseLevel(config['populationSize'],config['baselevelIterations'],\n",
    "                                                              config['X'],config['Y'],config['trainingFraction'], \n",
    "                                                              config['classifier'],lambda clf,train: clf.predict(train),config['classifierName'], \n",
    "                                                              config['datasetName'], config['flatHyperParameters'],config['crossValidations']), \n",
    "                                                    iterations=config['pyNMHHSteps'],\n",
    "                                                    initialTemp=config['SA_temp0'], cooling_rate= config['SA_coolingRate'])\n",
    "\n",
    "    print(f\"            >>>Best fitness achieved: {best_fitness}\")\n",
    "    # print(f\"Best {classifierName} hyperparameter history\")\n",
    "    # [print(f'Best hyperparameter step: {rvParamFlattener.unflatten(flatsvmparam)} -> frac: {frac}={accuracy} vs big: {totalSVMScore(rvParamFlattener.unflatten(flatsvmparam),X,y)} ') for flatsvmparam,accuracy in zip(history.best_offspring,history.function_values)]\n",
    "    # print(\"Optimized baseLevelConfig parameters:\")\n",
    "    # print(pyNMHH.json.dumps(best_params, indent=2))\n",
    "    print(f\"            >>>best sequence: {bestSequence}\" )\n",
    "    print(f'            >>>tuned  hyperparameters {config[\"flatHyperParameters\"].unflatten(best_solution)}')\n",
    "    return {\n",
    "                \"bestBaseConfig\": best_params,\n",
    "                \"bestAccuracy\":-best_fitness,\n",
    "                'solution':config[\"flatHyperParameters\"].unflatten(best_solution),\n",
    "                'bestSequence':bestSequence,\n",
    "            }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from timeit import default_timer as timer\n",
    "import numpy as np\n",
    "\n",
    "def runClassificationTuningExperiment(config):\n",
    "    solutions=[]\n",
    "    experimentTimes=[]\n",
    "    for i in range(config['classificationTuningCount']):\n",
    "        print(f'            >>>Running  iteration {i+1}/{config[\"classificationTuningCount\"]}\\n')\n",
    "        start = timer()\n",
    "        solutions.append(runClassificationTuning(config))\n",
    "        end = timer()\n",
    "        elapsed=end-start\n",
    "        experimentTimes.append(elapsed)\n",
    "        etaSeconds=(config['classificationTuningCount']-(i+1))*np.mean(experimentTimes)\n",
    "        print(f'            >>>Ran  {i+1}/{config[\"classificationTuningCount\"]} iterations elapsed seconds {elapsed} eta: {etaSeconds} seconds')\n",
    "    return solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runSVMClassification():\n",
    "    svm_params = {\n",
    "        'C': [1.0,50.0],\n",
    "        \"kernel\":['linear','poly','rbf','sigmoid']\n",
    "    }\n",
    "    digits=datasets.load_digits()\n",
    "    X,y=digits.data, digits.target\n",
    "    config={\n",
    "        'X':X,\n",
    "        'Y':y,\n",
    "        'flatHyperParameters':FlatHyperParameters(svm_params),\n",
    "        'classifier':svm.SVC,\n",
    "        'populationSize':5,\n",
    "        'baselevelIterations':10,\n",
    "        'crossValidations':3,\n",
    "        'pyNMHHSteps': 3,\n",
    "        'classificationTuningCount':7,\n",
    "        'classifierName':'SVM',\n",
    "        'datasetName':'Digits',\n",
    "        'trainingFraction':0.75,\n",
    "        'baseLevelConfig':initialClassificationBaseLevelConfig(),\n",
    "        'SA_temp0':1.0,\n",
    "        'SA_coolingRate':0.995\n",
    "    }\n",
    "    runClassificationTuningExperiment(config)\n",
    "   \n",
    "    \n",
    "# print(runSVMClassification())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def runRFClassification():\n",
    "    rf_params = {\n",
    "        'n_estimators': [10,100],\n",
    "        \"max_features\":[1,64],\n",
    "        'max_depth': [5,50],\n",
    "        \"min_samples_split\":[2,11],\n",
    "        \"min_samples_leaf\":[1,11],\n",
    "        \"criterion\":['gini','entropy']\n",
    "    }\n",
    "    digits=datasets.load_digits()\n",
    "    X,y=digits.data, digits.target\n",
    "    config={\n",
    "        'X':X,\n",
    "        'Y':y,\n",
    "        'flatHyperParameters':FlatHyperParameters(rf_params),\n",
    "        'classifier':RandomForestClassifier,\n",
    "        'populationSize':10,\n",
    "        'baselevelIterations':50,\n",
    "        'pyNMHHSteps': 3,\n",
    "        'crossValidations':3,\n",
    "        'classificationTuningCount':10,\n",
    "        'classifierName':'RandomForest',\n",
    "        'datasetName':'Digits',\n",
    "        'trainingFraction':0.75,\n",
    "        'baseLevelConfig':initialClassificationBaseLevelConfig(),\n",
    "        'SA_temp0':1.0,\n",
    "        'SA_coolingRate':0.995\n",
    "    }\n",
    "    runClassificationTuningExperiment(config)\n",
    "     \n",
    "# runRFClassification()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
